name: Run Python Bot

on:
  workflow_dispatch:  # Trigger manually for instant hacking
  schedule:
    - cron: '0 */3 * * *'  # Run every 3 hours to keep the digital underworld in check
  push:
    paths:
      - 'writeup-watcher-v2.py'  # Run only if bot script changes
      - 'requirements.txt'     # Run if dependencies change

jobs:
  run-cyber-bot:
    runs-on: ubuntu-latest
    timeout-minutes: 60  # 1-hour timeout to stay lean and mean

    steps:
    - name: Checkout the arsenal
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for stealth commits

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: 3.11

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install hacking tools
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run the CyberSentry bot
      env:
        TELEGRAM_TOKEN: ${{ secrets.TELEGRAM_TOKEN }}
        TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        WEBHOOK_URL: ${{ secrets.WEBHOOK_URL }}
      run: |
        python3 -u writeup-watcher-v2.py > bot_logs.txt || true  # Save logs and keep going if bot crashes

    - name: Clean old cache entries
      run: |
        python3 -c "
        import json
        import time
        CACHE_FILE = 'medium_cache.json'
        CACHE_EXPIRY = 86400  # 24 hours
        try:
            with open(CACHE_FILE, 'r') as f:
                cache = json.load(f)
            current_time = time.time()
            cleaned_cache = {k: v for k, v in cache.items() if (current_time - v['timestamp']) < CACHE_EXPIRY}
            with open(CACHE_FILE, 'w') as f:
                json.dump(cleaned_cache, f)
            print('Old cache entries cleaned.')
        except FileNotFoundError:
            print('No cache file found, skipping cleanup.')
        "

    - name: Commit and push intel
      uses: stefanzweifel/git-auto-commit-action@v5
      with:
        commit_message: "CyberSentry: Updated URL, post, and cache files"
        file_pattern: "twitter_urls.json medium_urls.json stored_urls.json medium_posts.json medium_cache.json bot_logs.txt"
        commit_user_name: "CyberSentry[bot]"
        commit_user_email: "cybersentry@users.noreply.github.com"
        commit_author: "CyberSentry <cybersentry@users.noreply.github.com>"

    - name: Notify mission status
      if: always()  # Run even if previous steps fail
      env:
        TELEGRAM_TOKEN: ${{ secrets.TELEGRAM_TOKEN }}
        TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
      run: |
        python3 -c "
        import telegram
        import asyncio
        async def send_notification():
            bot = telegram.Bot(token='$TELEGRAM_TOKEN')
            status = 'Success' if '${{ job.status }}' == 'success' else 'Failed'
            message = f'üéØ CyberSentry Mission Report üéØ\nStatus: {status}\nTime: ${{ github.event.schedule || 'Manual' }}\nNew intel committed to the vault.'
            await bot.send_message(chat_id='$TELEGRAM_CHAT_ID', text=message, parse_mode='Markdown')
        asyncio.run(send_notification())
        "
```
### Changes Made
1. **Scheduled Execution**: Added a `schedule` trigger to run the bot every 6 hours (`cron: '0 */6 * * *'`). This complements the `workflow_dispatch` for manual runs and ensures periodic execution without relying solely on manual triggers.
2. **Timeout Configuration**: Added `timeout-minutes: 360` to limit the job to 6 hours, aligning with GitHub Actions‚Äô default job timeout to prevent unexpected termination.
3. **Full Git History**: Set `fetch-depth: 0` in the checkout step to ensure the full repository history is available for committing changes.
4. **File Commit Step**: Added a step to commit and push `twitter_urls.json`, `medium_urls.json`, and `stored_urls.json` to the repository. The `|| echo "No changes to commit"` ensures the workflow doesn‚Äôt fail if there are no changes to commit.
5. **Error Tolerance**: Added `|| true` to the `Run bot script` step to ensure the workflow proceeds to the commit step even if the bot crashes or is interrupted (e.g., via `KeyboardInterrupt`).
6. **GitHub Token**: Uses `${{ secrets.GITHUB_TOKEN }}` for authentication when pushing changes, which is automatically provided by GitHub Actions and requires repository write permissions.

### Additional Notes
- **Permissions**: Ensure your repository settings allow GitHub Actions to write to the repository. You may need to enable ‚ÄúAllow GitHub Actions to create and approve pull requests‚Äù in the repository‚Äôs Actions settings or use a personal access token with `repo` scope if `GITHUB_TOKEN` is insufficient.
- **File Conflict Handling**: If multiple workflow runs occur concurrently, there‚Äôs a small risk of commit conflicts when updating the JSON files. Consider adding a lock mechanism or using a database (e.g., SQLite) if this becomes an issue.
- **Timeout Adjustment**: If 6 hours is too long or short, adjust `timeout-minutes` to your needs (e.g., 60 for 1 hour). Alternatively, you could modify the Python script to exit after a certain number of cycles.
- **Requirements**: Ensure all dependencies in `requirements.txt` (e.g., `requests`, `beautifulsoup4`, `python-telegram-bot`, `colorlog`, `tqdm`, `ntscraper`) are up-to-date and compatible with Python 3.11.
- **Testing**: Test the workflow manually via `workflow_dispatch` to ensure the bot runs and commits files correctly. Check the repository for updated JSON files after each run.

### Do You Need Further Changes?
- If you want to adjust the schedule (e.g., run every 3 hours instead of 6), let me know the desired frequency.
- If you prefer a different approach for persisting files (e.g., uploading to a cloud storage service instead of committing to the repo), I can modify the workflow accordingly.
- If you want to keep the workflow as-is and not make these changes, let me know, and I‚Äôll confirm that no updates are needed.
- If you want to re-enable the commented-out Reddit scraping code with similar file storage logic, I can include that in the Python script and update the workflow to handle `reddit_urls.json`.

Please confirm if this updated workflow meets your needs or specify any additional changes!
